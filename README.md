# üíªProjeto Datalake com Spark
- Este projeto demonstra a an√°lise de vendas usando PySpark dentro de um cont√™iner Docker. 
- O objetivo √© criar um DataFrame com um volume significativo de dados, realizar opera√ß√µes de agrega√ß√£o e exibir os resultados. 
- O ambiente de execu√ß√£o √© isolado e replic√°vel, utilizando Docker.


>**Grupo (Cascas de Bala)**
> 1. Dyelle Hemylle Nunes de Almeida
> 2. Ingrid Gabrielly Camara Lira
> 3. Jarmison Kecio Ferreira da Cunha
> 4. Kleber Lucas Lopes Alves
> 5. T√°llyson Emanoel Roques Izidio
> 6. Vitor Eduardo de Carvalho

## ‚úÖPr√©-requisitos
- Docker instalado na sua m√°quina. 
- [Site oficial](https://www.docker.com/)
para baixar e instalar o Docker.

## ‚ôüÔ∏èEstrutura do Projeto

```datalake-spark-project/
‚îú‚îÄ‚îÄ app/
‚îÇ ‚îî‚îÄ‚îÄ app.py
‚îî‚îÄ‚îÄ Dockerfile
‚îî‚îÄ‚îÄ README.md
‚îî‚îÄ‚îÄ package-lock.json
```

## ‚úÖConstruir imagem
Para construir a imagem Docker, execute o seguinte comando no diret√≥rio raiz do projeto (datalake-spark-project):
```bash
docker build -t datalake:latest .
```


## ‚úÖExecutar
Para executar o cont√™iner Docker com a imagem constru√≠da, use o comando:
```
docker run datalake
```

# üìÑDocumenta√ß√£o do Projeto

## Depend√™ncias do Projeto

Este projeto depende das seguintes tecnologias e bibliotecas:

- **Apache Spark**: uma estrutura de computa√ß√£o distribu√≠da para processamento de dados em grande escala.
- **PySpark**: a API Python para Apache Spark, permitindo a manipula√ß√£o de grandes conjuntos de dados.
- **Docker**: uma plataforma para desenvolver, enviar e executar aplica√ß√µes em cont√™ineres.

### Instala√ß√£o de Depend√™ncias

Certifique-se de que as seguintes depend√™ncias est√£o instaladas na sua m√°quina:

1. **Docker**:
   - Siga as instru√ß√µes de instala√ß√£o no [site oficial do Docker](https://www.docker.com/).

2. **Python** (opcional, apenas se voc√™ quiser executar os scripts localmente sem Docker):
   - Instale a vers√£o mais recente do Python a partir do [site oficial do Python](https://www.python.org/downloads/).
   - Instale o PySpark usando o pip:
     ```bash
     pip install pyspark
     ```

## Scripts e Programas
Esta se√ß√£o detalha os scripts e programas inclu√≠dos neste projeto, explicando sua funcionalidade e estrutura.
```
app.py:
``` 
- Este script realiza a an√°lise de vendas utilizando PySpark. 
- Ele cria um DataFrame com dados de produtos, quantidade e pre√ßo, calcula a receita total e a m√©dia de vendas por produto, e exibe os resultados.
- Inicializa a sess√£o do Spark para processar dados.

```
Dockerfile:
``` 
- Configura o ambiente necess√°rio para executar o script app.py em um cont√™iner Docker. 
- Instala Python e bibliotecas necess√°rias.
- Define o diret√≥rio de trabalho e copia os arquivos do projeto.
- O cont√™iner executa app.py ao iniciar.

## Execu√ß√£o Local (Opcional)

Se voc√™ preferir executar o script localmente sem usar Docker, voc√™ pode seguir estas etapas:

1. Certifique-se de que o PySpark est√° instalado (como mencionado anteriormente).
2. Navegue at√© o diret√≥rio onde o `app.py` est√° localizado.
3. Execute o script usando o comando:
   ```bash
   python app/app.py

## Exemplo de Sa√≠da
Ap√≥s executar o cont√™iner ou o script localmente, voc√™ ver√° uma sa√≠da semelhante a esta:

| Produto  | Quantidade | Preco| 
| ------------- | ------------- | ------------- | 
| Produto1  | 1 | 150 |
| Produto2  | 2 | 250 |
| Produto3  | 3 | 350 |
| ... | ... | ... |

### Receita Total por Produto:
| Produto  | Receita_Total |
| ------------- | ------------- | 
| Produto9  | 2755000 |
| Produto8  | 2380000 | 
| Produto5  | 1375000 | 
| ... | ... |

### M√©dia de Vendas por Produto:
| Produto  | Media_Vendas |
| ------------- | ------------- | 
| Produto9  |  27550.0 |
| Produto8  | 23800.0 | 
| Produto5  | 13750.0 | 
| ... | ... |
## Como o Projeto Funciona
1. **Inicializa√ß√£o da SparkSession:** o script come√ßa criando uma SparkSession, que √© a entrada principal para a funcionalidade de Spark.
2. **Cria√ß√£o do DataFrame:** gera um DataFrame com 1000 registros, cada um representando vendas de diferentes produtos.
3. **Opera√ß√µes de Agrega√ß√£o:** o script calcula a receita total e a m√©dia de vendas por produto, utilizando fun√ß√µes de agrega√ß√£o do PySpark.
4. **Exibi√ß√£o dos Resultados:** os resultados das opera√ß√µes de agrega√ß√£o s√£o exibidos no console.
5. **Execu√ß√£o em Docker:** o projeto √© executado dentro de um cont√™iner Docker para garantir um ambiente consistente e replic√°vel.

## Benef√≠cios do Uso de Docker
1. **Isolamento de Ambiente:** Docker fornece um ambiente de execu√ß√£o isolado, garantindo que as depend√™ncias e configura√ß√µes sejam consistentes em diferentes m√°quinas.
2. **Reprodutibilidade:** o uso de cont√™ineres garante que o c√≥digo funcione da mesma maneira em qualquer m√°quina, eliminando problemas de "funciona na minha m√°quina".
3. **Facilidade de Distribui√ß√£o:** compartilhar o cont√™iner Docker com outras pessoas permite que elas executem o projeto sem precisar configurar o ambiente manualmente.

## Benef√≠cios de construir um datalake utilizando Apache Spark
1. **Processamento de Alta Velocidade:**
Spark executa opera√ß√µes em mem√≥ria sempre que poss√≠vel, acelerando significativamente o processamento em compara√ß√£o com frameworks baseados em disco como o Hadoop MapReduce.
2. **Escalabilidade:** Spark √© projetado para escalar horizontalmente, facilitando a adi√ß√£o de n√≥s ao cluster para lidar com grandes volumes de dados e aumentar a capacidade de processamento. Funciona em uma variedade de ambientes (incluindo on-premises, AWS, Google Cloud, Azure, etc.).
3. **Versatilidade:** Suporta m√∫ltiplas linguagens (Python, Java, Scala, R).
Tem bibliotecas integradas para SQL, machine learning, processamento de grafos e streaming.
4. **Integra√ß√£o com Diversas Fontes de Dados:** Conectores para HDFS, S3, Google Cloud Storage, bancos de dados SQL e NoSQL, Kafka, entre outros.
5. **Facilidade de Uso e Desenvolvimento R√°pido:**
Oferece APIs de alto n√≠vel f√°ceis de usar e entender, promovendo desenvolvimento r√°pido e eficiente. Conta com uma shell interativa para Scala e Python, permitindo desenvolvimento e experimenta√ß√£o iterativa com dados.
6. **Robustez e Toler√¢ncia a Falhas:**
Lida eficientemente com falhas de n√≥s, reprocessando opera√ß√µes a partir de dados j√° processados (RDDs). Tem uma comunidade ativa de desenvolvedores e amplo suporte, incluindo documenta√ß√£o detalhada, tutoriais e f√≥runs.
7. **Custo-Efetividade:**
Projeto de c√≥digo aberto sem custos de licenciamento, podendo ser executado em hardware comum ou na nuvem, proporcionando flexibilidade de custos. Uso eficiente de recursos como processamento em mem√≥ria e escalabilidade horizontal reduz custos operacionais.
8. **Capacidade de Processar Dados Estruturados e N√£o Estruturados:**
Processa uma ampla variedade de tipos de dados, incluindo estruturados (tabelas SQL), semiestruturados (JSON, XML) e n√£o estruturados (logs, m√≠dia).


